{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "키워드추출&그룹핑&대표키워드뽑기.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhnh9E20GEmY"
      },
      "source": [
        "# 키워드 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5lhj_FGIa_"
      },
      "source": [
        "## TF-IDF를 이용한 keyword 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVo9tA9ZBcgX"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "def komoran_tokenize(sents):\n",
        "  i=0\n",
        "  word=[]\n",
        "  for sent in sents:\n",
        "    i+=1\n",
        "    words = komoran.pos(sent, join=True)\n",
        "    words = [w.split(\"/\")[0].lower() for w in words if ('/NN' in w or '/XR' in w or '/VA' in w) and len(w.split(\"/\")[0]) > 1] #명사, 동사, 형용사   #or '/VV' in w\n",
        "    word.append(\" \".join(words))\n",
        "  return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBc4Ak6vMLtc",
        "outputId": "335bbcea-84b8-4c07-895c-60aba858f1b4"
      },
      "source": [
        "import konlpy\n",
        "\n",
        "for z in range(0,1):\n",
        "  feature0=perhotel[z]['sentence']\n",
        "  nouns0=[]\n",
        "  nouns1=[]\n",
        "  twitter=Twitter()\n",
        "  \n",
        "  #twitter 형태소 사용\n",
        "  for sentence in feature0:\n",
        "    if sentence is not '':\n",
        "      nouns0.append(' '.join([noun for noun in twitter.nouns(str(sentence))if noun not in stopwords and len(noun) > 1]))\n",
        "\n",
        "  \n",
        "  nouns0=komoran_tokenize(nouns0)\n",
        "\n",
        "  cnt_vec = TfidfVectorizer()\n",
        "  cnt_vec_mat = normalize(cnt_vec.fit_transform(nouns0).toarray().astype(float), axis=0)\n",
        "  vocab0 = cnt_vec.vocabulary_\n",
        "  words_graph0=np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
        "  \n",
        "  idx2word0={vocab0[word] : word for word in vocab0}\n",
        "\n",
        "  A = words_graph0\n",
        "  d=0.85\n",
        "  matrix_size = A.shape[0]\n",
        "  for id in range(matrix_size):\n",
        "    A[id, id] = 0 # diagonal 부분을 0으로\n",
        "    link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
        "      \n",
        "    if link_sum != 0:\n",
        "        A[:, id] /= link_sum\n",
        "    A[:, id] *= -d\n",
        "    A[id, id] = 1\n",
        "  B = (1-d) * np.ones((matrix_size, 1))\n",
        "  ranks = np.linalg.solve(A, B)\n",
        "  rank_idx={idx: r[0] for idx, r in enumerate(ranks)}\n",
        "\n",
        "  sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
        "  \n",
        "  keywords = []\n",
        "  index=[]\n",
        "  for idx in sorted_rank_idx[:30]:\n",
        "    index.append(idx)\n",
        "  \n",
        "  #index.sort()\n",
        "  for idx in index:\n",
        "    keywords.append(di.loc[di['0']==idx,'1'])\n",
        "  \n",
        "  keywords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcWJTfxpIvxk"
      },
      "source": [
        "## 그룹핑하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZAHCnhtIyCM"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "feature0_0=perhotel[0]\n",
        "feature0_0=feature0_0.drop(['level_0'],axis=1)\n",
        "feature0_0=feature0_0.reset_index()\n",
        "\n",
        "hotel0 = [\n",
        "    '깔끔', '가성비', '깨끗', '강남', '가격', '위치', '직원', '이용', '친절', '먼지가', '시설', '만족',\n",
        "    '출장', '생각보다', '없어서', '주변에', '보증금', '객실', '주차', '가까워서', '화장실', '레지던스',\n",
        "    '오피스텔', '프런트', '청결', '접근성', '별로', '칫솔', '샴푸', '수건', '편의', '없는', '대비',\n",
        "    '가깝고', '청소', '조용', '저렴', '느낌', '음식', '편하게', '방이', '취사', '아쉬', '어반', '불편',\n",
        "    '서비스', '교통', '방도', '침구', '냄비', '엘리베이터', '가능', '어메니티'\n",
        "] #각 호텔 키워드\n",
        "\n",
        "\n",
        "nouns=[]\n",
        "nouns=komoran_tokenize(feature0_0['sentence'])\n",
        "embedding_model = Word2Vec(nouns, min_count=3,window=3,iter=20, size=100, sg=1)\n",
        "embedding_model.wv.syn0.shape\n",
        "word_vectors = embedding_model.wv.syn0\n",
        "\n",
        "\n",
        "for z in range (0,len(embedding_model.wv.index2word)):\n",
        "  if embedding_model.wv.index2word[z] in embedding_dict.keys():\n",
        "      word_vectors[z]=embedding_dict[embedding_model.wv.index2word[z]]\n",
        "  else:\n",
        "    print(\"N\")\n",
        "\n",
        "word_vec=[]\n",
        "name=[]\n",
        "for z in range (0,len(embedding_model.wv.index2word)):\n",
        "  if embedding_model.wv.index2word[z] in hotel0:\n",
        "    word_vec.append(word_vectors[z])\n",
        "    name.append(embedding_model.wv.index2word[z])\n",
        "\n",
        "n=17 #로 미리 군집 개수 설정\n",
        "num_clusters=16\n",
        "kmeans_clustering = KMeans( n_clusters = num_clusters )#defaul\n",
        "idx = kmeans_clustering.fit_predict(word_vectors) #각 단어가 어떤 군집에 속하는지 확인\n",
        "\n",
        "idx = list(idx)\n",
        "names = embedding_model.wv.index2word #학습된 단어들 unique\n",
        "word_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n",
        "\n",
        "print(\"--------------------Hotel-------------------\")\n",
        "print(i)\n",
        "answer=[]\n",
        "for cluster in range(0,16):\n",
        "  words = []\n",
        "  #list(dict.values)는 value값이 리스트로 반환\n",
        "  #list(dict)키 값만 리스트로 반환\n",
        "  for i in range(0,len(list(word_centroid_map.values()))):\n",
        "    if( list(word_centroid_map.values())[i] == cluster ):\n",
        "      words.append(list(word_centroid_map.keys())[i])\n",
        "  answer.append(words)\n",
        "print(answer)\n",
        "print(\"--------------------Next-------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ4XQO6HKuPD"
      },
      "source": [
        "## 대표 뽑기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbuhuq3NKuyq"
      },
      "source": [
        "nouns1=komoran_tokenize1(feature0_0['sentence'])\n",
        "print(nouns1)\n",
        "tfidfv = TfidfVectorizer().fit(nouns1)\n",
        "tfidf=tfidfv.transform(nouns1).toarray()\n",
        "\n",
        "dictionary = {} #대표 단어 key, 대표 단어가 속한 그룹 value\n",
        "for i in range(len(hotel0_k)): \n",
        "  # hotel0_k는 단어 그룹\n",
        "  # [['방문'],['객실', '침대', '발코니'],['접근성', '주차', '로비', '편의', '공용'], ['방음'], ['가격', '업그레이드'],['컨디션'], \n",
        "  # ['현대', '시티'], ['위치', '주변'],['이용', '서비스'], ['코로나'],['친절', '청결'], ['시설', '건물'], ['교통'], \n",
        "  # ['프런트', '체크인', '아웃렛'],  ['동대문'],  ['깨끗', '깔끔'], ['직원', '의사']]\n",
        "  if len(hotel0_k[i])>=2: # 한 그룹의 단어 갯수가 2개 이상일 경우 대표 단어 추출 과정 필요 \n",
        "    #한 그룹의 각 단어의 TF-IDF의 값과 Word2Vec에서 제공해주는 다른 단어와의 유사도 값을 합산하여 최고점의 단어를 뽑기 위해\n",
        "    #한 그룹의 단어 수 만큼 배열을 만듦(배열 ar)\n",
        "    #https://www.koreascience.or.kr/article/JAKO201833469090754.pdf (p28)\n",
        "    ar=[0]*len(hotel0_k[i])\n",
        "    #hotel24에 hotel0_k의 i번째 그룹의 단어가 포함되어 있는 문장수 체크(c2에 저장)\n",
        "    for j in range(len(hotel0_k[i])):\n",
        "      c2=0\n",
        "      for c in feature0_0['sentence']: #hotel24의 sentence(feature0_0['sentence'])\n",
        "        for c1 in hotel0_k[i]:\n",
        "          if c1 in c:\n",
        "            c2+=1\n",
        "            break\n",
        "      #hotel0_k에서 i번째 그룹의 j번째 단어가 vocabulary 몇 번째에 있는지 z에 저장\n",
        "      z=tfidfv.vocabulary_[hotel0_k[i][j]]\n",
        "      #hotel0_k에서 i번째 그룹의 j번째 단어의 TF-IDF의 값(t에 저장)\n",
        "      t=0\n",
        "      for j1 in range(len(tfidf)):\n",
        "        t+=tfidf[j1][z]\n",
        "      #t=t/len(tfidf)\n",
        "      t=t/c2\n",
        "      #hotel0_k에서 i번째 그룹의 j번째 단어가 i번째 그룹의 다른 단어와의 유사도 값 합산(t1에 저장)\n",
        "      t1=0\n",
        "      for j1 in range(len(hotel0_k[i])):\n",
        "        if j1 != j:\n",
        "          t1+=embedding_model.wv.similarity(w1=hotel0_k[i][j], w2=hotel0_k[i][j1])\n",
        "      t1=t1/(len(hotel0_k[i])-1)\n",
        "      #배열 ar의 j번째에 j번째의 TF-IDF값과 유사도 값을 합산한 값을 저장\n",
        "      ar[j]=t+t1\n",
        "    #가장 큰 합산 값을 대표 단어로 선정\n",
        "    dictionary[hotel0_k[i][ar.index(max(ar))]]=hotel0_k[i] #dictionary[key]=value\n",
        "  else: # 한 그룹의 단어 갯수가 1개면 그것이 대표 단어\n",
        "    dictionary[hotel0_k[i][0]]=hotel0_k[i]\n",
        "print(dictionary) #hotel24\n",
        "#{'방문': ['방문'], '발코니': ['객실', '침대', '발코니'], '편의': ['접근성', '주차', '로비', '편의', '공용'], \n",
        "#'방음': ['방음'], '가격': ['가격', '업그레이드'], '컨디션': ['컨디션'], \n",
        "#'시티': ['현대', '시티'], '위치': ['위치', '주변'], '이용': ['이용', '서비스'], \n",
        "#'코로나': ['코로나'], '친절': ['친절', '청결'],'시설': ['시설', '건물'], '교통': ['교통'], \n",
        "#'프런트': ['프런트', '체크인', '아웃렛'], '동대문': ['동대문'], '깨끗': ['깨끗', '깔끔'], '직원': ['직원', '의사']}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}