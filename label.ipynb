{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"label.ipynb","provenance":[{"file_id":"1YdGp_lYv8DQL9RXWXuGs6896xUliLY8v","timestamp":1620778668997}],"collapsed_sections":[],"mount_file_id":"1pAFRAnoENjvVpXVLwKwdEXjVvMKX6H2J","authorship_tag":"ABX9TyMDvaSBFgzGS5n4rKGUq8+X"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mJxLzGwk4xZv"},"source":["! pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvR6yXMd2hYu"},"source":["!pip install -U imbalanced-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKqY7qXW3-OX"},"source":["import pandas as pd\n","from konlpy.tag import Okt;\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObV-a-on5Urf"},"source":["def tokenizer(text):\n","  okt = Okt()\n","  return okt.morphs(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwEj-IxL0ZOj"},"source":["def load_data(path):\n","  df = pd.read_csv(path)\n","  df = df[df.label != 2] \n","\n","  text_list = df['sentence'].tolist()\n","  label_list = df['label'].tolist()\n","\n","  return text_list, label_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-dOpE2Dr9Vy"},"source":["def split(text_list, label_list):\n","\n","  from sklearn.model_selection import train_test_split\n","\n","  text_train, text_test, label_train, label_test = train_test_split(text_list, label_list, test_size=0.2, random_state=42)\n","\n","  return text_train, text_test, label_train, label_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqlZa0Nms_Rs"},"source":["def learn(X_train, X_test, y_train, y_test, model):\n","  \n","  # 데이터 단어사전 -> 백터화\n","  from sklearn.feature_extraction.text import TfidfVectorizer\n","  tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n","\n","  #from sklearn.linear_model import LogisticRegression\n","  #logistic = LogisticRegression(C=10.0, penalty='l2', random_state=0)\n","\n","  from sklearn.svm import LinearSVC\n","  svm = LinearSVC()\n","\n","  from sklearn.pipeline import Pipeline\n","  pipe = Pipeline([('vect', tfidf), ('clf', svm)])\n","\n","  pipe.fit(X_train, y_train)\n","\n","  from sklearn.metrics import accuracy_score\n","  from sklearn.metrics import classification_report\n","  y_pred = pipe.predict(X_test)\n","  print(accuracy_score(y_test, y_pred))\n","  print(classification_report(y_test, y_pred))\n","\n","  with open(model, 'wb') as fp:\n","    pickle.dump(pipe, fp)\n","  \n","  print('Model Saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQwGCV8_uL5J"},"source":["def test(model):\n","\n","  with open(model, 'rb') as fp:\n","    pipe = pickle.load(fp)\n","  \n","  import numpy as np\n","\n","  while True:\n","    text = input('리뷰를 작성해주세요: ')\n","    if text == 'end':\n","      break\n","    str = [text]\n","    r1 = np.max(pipe.predict_proba(str)*100)\n","    r2 = pipe.predict(str)[0]\n","    print('acc =', r1)\n","    print(r2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kl2MFJs1fe0"},"source":["def tonumpy(text_list, label_list):\n","  \n","  import numpy as np\n","  X_imb, y_imb = np.array(text_list), np.array(label_list)\n","  X_imb, y_imb = X_imb.reshape((-1, 1)), y_imb.reshape((-1, 1))\n","\n","  return X_imb, y_imb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Bk8M6zu1_oJ"},"source":["def toList(X_samp):\n","  a = []\n","  for i in X_samp:\n","    a.append(i[0])\n","  return a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xGC0VxTvSjk"},"source":["def under_sampling(text_list, label_list):\n","\n","  X_imb, y_imb = tonumpy(text_list, label_list)\n","\n","  from imblearn.under_sampling import RandomUnderSampler\n","  X_samp, y_samp = RandomUnderSampler(random_state=0).fit_resample(X_imb, y_imb)\n","\n","  return toList(X_samp), y_samp.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCZXNloB5YYQ"},"source":["def over_sampling(text_list, label_list):\n","  X_imb, y_imb = tonumpy(text_list, label_list)\n","\n","  from imblearn.over_sampling import RandomOverSampler\n","  X_samp, y_samp = RandomOverSampler(random_state=0).fit_resample(X_imb, y_imb)\n","\n","  return toList(X_samp), y_samp.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwh5YSWp2eKa"},"source":["def train(model, mode):\n","  t, l = load_data(path)\n","  if mode == -1:\n","    t, l = under_sampling(t, l)\n","  if mode == 1:\n","    t, l = over_sampling(t, l)\n","  text_train, text_test, label_train, label_test = split(t, l)\n","  learn(text_train, text_test, label_train, label_test, model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6VsjQK79vux"},"source":["def label_csv(model,c_path):\n","  df = pd.read_csv(path)\n","  df = df[df['sentence'].notna()]\n","\n","  text_list = df['fixed'].tolist()\n","\n","  with open(model, 'rb') as fp:\n","    pipe = pickle.load(fp)\n","\n","  import numpy as np\n","\n","  p_label = []\n","  p_proba = []\n","\n","  for text in text_list:\n","    str = [text]\n","    p_proba.append(np.max(pipe.predict_proba(str)*100))\n","    p_label.append(pipe.predict(str)[0])\n","\n","  df['label'] = p_label\n","  df['probability'] = p_proba \n","\n","  df.to_csv('spell_check_label.csv')\n","  print('csv saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxOQmX6CMPiO"},"source":["df = pd.read_csv('/content/drive/MyDrive/spell_check_label.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0biYv124rLm","executionInfo":{"status":"ok","timestamp":1620912499709,"user_tz":-540,"elapsed":40315,"user":{"displayName":"이재리","photoUrl":"","userId":"07799536652921530236"}},"outputId":"b4aab12e-e467-4913-da51-501420676065"},"source":["path = '/content/drive/MyDrive/spell_check.csv'\n","train('none_ori', 0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9399974946761869\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.83      0.84      1485\n","           1       0.96      0.97      0.96      6498\n","\n","    accuracy                           0.94      7983\n","   macro avg       0.90      0.90      0.90      7983\n","weighted avg       0.94      0.94      0.94      7983\n","\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYt2S5le8Rvl","executionInfo":{"status":"ok","timestamp":1620782581546,"user_tz":-540,"elapsed":156358,"user":{"displayName":"이재리","photoUrl":"","userId":"07799536652921530236"}},"outputId":"245f2d43-96f4-4061-d4ff-052295e8846e"},"source":["path = '/content/drive/MyDrive/spell_check_unlabel.csv'\n","\n","#oversampling mode = 1\n","train('oversamp.dat', 1)\n","\n","# 테스트 - end 입력시 종료\n","#test('/content/drive/MyDrive/models/over.dat')\n","\n","# 파일 레이블\n","c_path = '/content/drive/MyDrive/spell_check_unlabel.csv'\n","label_csv('/content/drive/MyDrive/models/over.dat', c_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["csv saved\n"],"name":"stdout"}]}]}